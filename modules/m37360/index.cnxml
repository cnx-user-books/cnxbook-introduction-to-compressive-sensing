<document xmlns="http://cnx.rice.edu/cnxml" xmlns:m="http://www.w3.org/1998/Math/MathML">
  <title>Linear regression and model selection</title>
<metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m37360</md:content-id>
  <md:title>Linear regression and model selection</md:title>
  <md:abstract>This module provides a brief overview of the relationship between model selection, sparse linear regression, and the techniques developed in compressive sensing.</md:abstract>
  <md:uuid>756afc37-9e13-40cd-8570-e84b70d9b7b6</md:uuid>
</metadata>

<content>
    <para id="id68401">Many of the <link document="m37292" version="latest">sparse recovery algorithms</link> we have described so far in this <link document="col11133" version="latest">course</link> were originally developed to address the problem of sparse linear regression and model selection in statistics. In this setting we are given some data consisting of a set of input variables and response variables. We will suppose that there are a total of <m:math overflow="scroll"><m:mi>N</m:mi></m:math> input variables, and we observe a total of <m:math overflow="scroll"><m:mi>M</m:mi></m:math> input and response pairs. We can represent the set of input variable observations as an <m:math overflow="scroll"><m:mrow><m:mi>M</m:mi><m:mo>×</m:mo><m:mi>N</m:mi></m:mrow></m:math> matrix <m:math overflow="scroll"><m:mi>Φ</m:mi></m:math>, and the set of response variable observations as an <m:math overflow="scroll"><m:mrow><m:mi>M</m:mi><m:mo>×</m:mo><m:mn>1</m:mn></m:mrow></m:math> vector <m:math overflow="scroll"><m:mi>y</m:mi></m:math>.</para>
    <para id="id68810">In linear regression, it is assumed that <m:math overflow="scroll"><m:mi>y</m:mi></m:math> can be approximated as a linear function of the input variables, i.e., there exists an <m:math overflow="scroll"><m:mi>x</m:mi></m:math> such that <m:math overflow="scroll"><m:mrow><m:mi>y</m:mi><m:mo>≈</m:mo><m:mi>Φ</m:mi><m:mi>x</m:mi></m:mrow></m:math>. However, when the number of input variables is large compared to the number of observations, i.e., <m:math overflow="scroll"><m:mrow><m:mi>M</m:mi><m:mo>≪</m:mo><m:mi>N</m:mi></m:mrow></m:math>, this becomes extremely challenging because we wish to estimate <m:math overflow="scroll"><m:mi>N</m:mi></m:math> parameters from far fewer than <m:math overflow="scroll"><m:mi>N</m:mi></m:math> observations. In general this would be impossible to overcome, but in practice it is common that only a few input variables are actually necessary to predict the response variable. In this case the <m:math overflow="scroll"><m:mi>x</m:mi></m:math> that we wish to estimate is sparse, and we can apply all of the techniques that we have learned so far for sparse recovery to estimate <m:math overflow="scroll"><m:mi>x</m:mi></m:math>. In this setting, not only does sparsity aid us in our goal of obtaining a regression, but it also performs <emphasis effect="italics">model selection</emphasis> by identifying the most relevant variables in predicting the response.</para>
  </content>
</document>